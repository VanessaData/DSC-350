{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0263cf46-6e4e-4179-b6a8-5051735af931",
   "metadata": {},
   "source": [
    "# Vanessa Williams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5ad16-2183-4382-8413-a9ec2f56c36d",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d55e-4eaf-4c34-b015-fd626542e73b",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a226eff-a388-41ce-9433-98de03244a31",
   "metadata": {},
   "source": [
    "#### Parts A & B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf33a14-e158-48ab-9996-b672d95e3e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date        high         low        open       close    volume  \\\n",
      "0    2018-01-02  201.649994  195.419998  196.100006  201.070007  10966900   \n",
      "1    2018-01-03  206.210007  201.500000  202.050003  205.050003   8591400   \n",
      "2    2018-01-04  207.050003  204.000000  206.199997  205.630005   6029600   \n",
      "3    2018-01-05  210.020004  205.589996  207.250000  209.990005   7033200   \n",
      "4    2018-01-08  212.500000  208.440002  210.020004  212.050003   5580200   \n",
      "..          ...         ...         ...         ...         ...       ...   \n",
      "246  2018-12-24  250.649994  233.679993  242.000000  233.880005   9547600   \n",
      "247  2018-12-26  254.500000  231.229996  233.919998  253.669998  14402700   \n",
      "248  2018-12-27  255.589996  240.100006  250.110001  255.570007  12235200   \n",
      "249  2018-12-28  261.910004  249.800003  257.940002  256.079987  10992800   \n",
      "250  2018-12-31  270.100006  260.000000  260.160004  267.660004  13508900   \n",
      "\n",
      "    ticker  \n",
      "0     AAPL  \n",
      "1     AAPL  \n",
      "2     AAPL  \n",
      "3     AAPL  \n",
      "4     AAPL  \n",
      "..     ...  \n",
      "246   AAPL  \n",
      "247   AAPL  \n",
      "248   AAPL  \n",
      "249   AAPL  \n",
      "250   AAPL  \n",
      "\n",
      "[251 rows x 7 columns] ['AMZN'] ['FB'] ['GOOG'] ['NFLX']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to read CSV files and add a ticker column\n",
    "def read_and_add_ticker(filenames):\n",
    "    dataframes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # Derive the ticker symbol from the filename by removing the '.csv'\n",
    "        ticker = filename.replace('.csv', '').upper()  # Assuming filenames are lowercase\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv('aapl.csv')\n",
    "        df = pd.read_csv('amzn.csv')\n",
    "        df = pd.read_csv('fb.csv')\n",
    "        df = pd.read_csv('goog.csv')\n",
    "        df = pd.read_csv('nflx.csv')\n",
    "        \n",
    "        # Add a 'ticker' column with the ticker symbol\n",
    "        df['ticker'] = ticker\n",
    "        \n",
    "        # Store the DataFrame in a dictionary\n",
    "        dataframes[ticker] = df\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "# Filenames to read in (assuming the files are in the current directory)\n",
    "filenames = ['aapl.csv', 'amzn.csv', 'fb.csv', 'goog.csv', 'nflx.csv']\n",
    "\n",
    "# Call the function and store the dataframes\n",
    "dataframes_with_tickers = read_and_add_ticker(filenames)\n",
    "\n",
    "# You can now access each dataframe by its ticker symbol, for example:\n",
    "print(dataframes_with_tickers['AAPL'], ['AMZN'], ['FB'], ['GOOG'], ['NFLX'])  # Display AAPL DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5b598-8aa3-4a03-85cf-62a4188147ad",
   "metadata": {},
   "source": [
    "#### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3995c281-eec0-46cd-a255-6d7add6716f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date        high         low        open       close       volume  \\\n",
      "0     2018-01-02   43.075001   42.314999   42.540001   43.064999  102223600.0   \n",
      "1     2018-01-03   43.637501   42.990002   43.132500   43.057499  118071600.0   \n",
      "2     2018-01-04   43.367500   43.020000   43.134998   43.257500   89738400.0   \n",
      "3     2018-01-05   43.842499   43.262501   43.360001   43.750000   94640000.0   \n",
      "4     2018-01-08   43.902500   43.482498   43.587502   43.587502   82271200.0   \n",
      "...          ...         ...         ...         ...         ...          ...   \n",
      "1250  2018-12-24  250.649994  233.679993  242.000000  233.880005    9547600.0   \n",
      "1251  2018-12-26  254.500000  231.229996  233.919998  253.669998   14402700.0   \n",
      "1252  2018-12-27  255.589996  240.100006  250.110001  255.570007   12235200.0   \n",
      "1253  2018-12-28  261.910004  249.800003  257.940002  256.079987   10992800.0   \n",
      "1254  2018-12-31  270.100006  260.000000  260.160004  267.660004   13508900.0   \n",
      "\n",
      "     ticker  \n",
      "0      AAPL  \n",
      "1      AAPL  \n",
      "2      AAPL  \n",
      "3      AAPL  \n",
      "4      AAPL  \n",
      "...     ...  \n",
      "1250   NFLX  \n",
      "1251   NFLX  \n",
      "1252   NFLX  \n",
      "1253   NFLX  \n",
      "1254   NFLX  \n",
      "\n",
      "[1255 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to read CSV files, add a ticker column, and append them into a single DataFrame\n",
    "def read_and_append_data(filenames):\n",
    "    dataframes = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # Derive the ticker symbol from the filename by removing the '.csv'\n",
    "        ticker = filename.replace('.csv', '').upper()  # Assuming filenames are lowercase\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Add a 'ticker' column with the ticker symbol\n",
    "        df['ticker'] = ticker\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Filenames to read in (assuming the files are in the current directory)\n",
    "filenames = ['aapl.csv', 'amzn.csv', 'fb.csv', 'goog.csv', 'nflx.csv']\n",
    "\n",
    "# Call the function to get the combined dataframe\n",
    "combined_dataframe = read_and_append_data(filenames)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(combined_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191fb4f1-6dac-4b8a-907f-daf89bb94c7c",
   "metadata": {},
   "source": [
    "#### Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34614e84-8053-484f-a379-7f1e637a1db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data has been saved to 'faang.csv'\n"
     ]
    }
   ],
   "source": [
    "combined_dataframe.to_csv('faang.csv', index=False)\n",
    "\n",
    "# Optionally, print a message to confirm it's saved\n",
    "print(\"Combined data has been saved to 'faang.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c7efe-02d3-4a7f-b77f-9ff5051df10e",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06c76713-d0a9-427e-a384-7edc8408530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date         high          low         open        close  \\\n",
      "0    2018-01-02    43.075001    42.314999    42.540001    43.064999   \n",
      "251  2018-01-02  1190.000000  1170.510010  1172.000000  1189.010010   \n",
      "502  2018-01-02   181.580002   177.550003   177.679993   181.419998   \n",
      "753  2018-01-02  1066.939941  1045.229980  1048.339966  1065.000000   \n",
      "1004 2018-01-02   201.649994   195.419998   196.100006   201.070007   \n",
      "\n",
      "         volume ticker  \n",
      "0     102223600   AAPL  \n",
      "251     2694500   AMZN  \n",
      "502    18151900     FB  \n",
      "753     1237600   GOOG  \n",
      "1004   10966900   NFLX  \n"
     ]
    }
   ],
   "source": [
    "# Read the faang.csv file into a DataFrame\n",
    "faang_df = pd.read_csv('faang.csv')\n",
    "\n",
    "# Convert the 'date' column to datetime type\n",
    "faang_df['date'] = pd.to_datetime(faang_df['date'])\n",
    "\n",
    "# Convert the 'volume' column to integer type\n",
    "faang_df['volume'] = faang_df['volume'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by 'date' and 'ticker'\n",
    "faang_df = faang_df.sort_values(by=['date', 'ticker'])\n",
    "\n",
    "# Optionally, print a few rows to confirm\n",
    "print(faang_df.head())\n",
    "\n",
    "# Save the sorted DataFrame back to CSV (if required)\n",
    "faang_df.to_csv('faang_sorted.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68948d-4fe5-4e89-8412-ff7fe60a2a4b",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3232427f-3788-48b7-a2d2-3a3fcd2cd8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date         high          low         open        close  volume  \\\n",
      "879 2018-07-03  1135.819946  1100.020020  1135.819946  1102.890015  679000   \n",
      "979 2018-11-23  1037.589966  1022.398987  1030.000000  1023.880005  691500   \n",
      "852 2018-05-24  1080.469971  1066.150024  1079.000000  1079.239990  766800   \n",
      "883 2018-07-10  1159.589966  1149.589966  1156.979980  1152.839966  798400   \n",
      "905 2018-08-09  1255.541992  1246.010010  1249.900024  1249.099976  848600   \n",
      "912 2018-08-20  1211.000000  1194.625977  1205.020020  1207.770020  870800   \n",
      "914 2018-08-22  1211.839966  1199.000000  1200.000000  1207.329956  887400   \n",
      "\n",
      "    ticker  \n",
      "879   GOOG  \n",
      "979   GOOG  \n",
      "852   GOOG  \n",
      "883   GOOG  \n",
      "905   GOOG  \n",
      "912   GOOG  \n",
      "914   GOOG  \n"
     ]
    }
   ],
   "source": [
    "# Convert the 'volume' column to integer type (in case it is not already)\n",
    "faang_df['volume'] = faang_df['volume'].astype(int)\n",
    "\n",
    "# Find the seven rows with the lowest values in the 'volume' column\n",
    "lowest_volume_rows = faang_df.nsmallest(7, 'volume')\n",
    "\n",
    "# Display the results\n",
    "print(lowest_volume_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fc8fb-455c-4d40-9ac4-35f1906039ac",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e73751d-3dc7-433a-88ec-a7a06fd83bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date ticker variable         value\n",
      "0    2018-01-02   AAPL     open  4.254000e+01\n",
      "1    2018-01-02   AMZN     open  1.172000e+03\n",
      "2    2018-01-02     FB     open  1.776800e+02\n",
      "3    2018-01-02   GOOG     open  1.048340e+03\n",
      "4    2018-01-02   NFLX     open  1.961000e+02\n",
      "...         ...    ...      ...           ...\n",
      "6270 2018-12-31   AAPL   volume  1.400140e+08\n",
      "6271 2018-12-31   AMZN   volume  6.954500e+06\n",
      "6272 2018-12-31     FB   volume  2.462530e+07\n",
      "6273 2018-12-31   GOOG   volume  1.493300e+06\n",
      "6274 2018-12-31   NFLX   volume  1.350890e+07\n",
      "\n",
      "[6275 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Melt the DataFrame to convert it into a long format\n",
    "long_format_df = pd.melt(faang_df, id_vars=['date', 'ticker'], \n",
    "                         value_vars=['open', 'high', 'low', 'close', 'volume'],\n",
    "                         var_name='variable', value_name='value')\n",
    "\n",
    "# Display the transformed long format DataFrame\n",
    "print(long_format_df)\n",
    "\n",
    "# Optionally, save the long format DataFrame to a CSV file\n",
    "long_format_df.to_csv('faang_long_format.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f306b-4243-42e5-b022-33c4be522d2d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1cf63-a5b4-406a-af71-cc7741c68fa7",
   "metadata": {},
   "source": [
    "#### Q: Suppose we found out that on July 26, 2018 there was a glitch in how the data wasrecorded. How should we handle this? Note that there is no coding required for thisexercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080c119-ab4e-40b3-ac5d-73dac4a894cb",
   "metadata": {},
   "source": [
    "The steps include understanding the extent of the issue, cross-checking with external sources, making an informed decision about how to handle the data, documenting your decision, and ensuring consistency across related processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
